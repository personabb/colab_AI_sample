{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9gYaN1CCXDfYicp7TrSDI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#ローカルLLMのサンプル利用コード\n","##利用方法\n","・基本的には全てのセルを実行することで利用できます。\n","\n","・cpuインスタンスのまま利用することを推奨します。\n","（GPUを利用した方が高速です）\n","\n","（変更する際は，「ランタイム」→「ランタイムのタイプを変更」からGPUを選択してください）\n","\n","##注意点\n","・本ファイルの相対的な場所は変更しないでください。\n","\n","・本ファイルの名前を変更しないでください。\n","\n","変更する場合は，[2]セル目のファイル名も一緒に変更してください。\n","\n","・[1]セル目の実行時にセッションの再起動を求められることがあります。\n","\n","その場合はポップアップに従って再起動したあと，全てのセルを再度実行してください。"],"metadata":{"id":"2MLPf4DBnsgU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVAhS-kHalqS","collapsed":true},"outputs":[],"source":["#ローカルLLM で必要なモジュールのインストール\n","\n","!set LLAMA_CUBLAS=1\n","!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n","!set FORCE_CMAKE=1\n","!python -m pip install llama-cpp-python==0.2.77 --prefer-binary  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n","!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"]},{"cell_type":"markdown","source":["下記のコードを実行すると，認証が求められるので，認証してください。（Google colabにマイドライブの中を読めるようにする認証です）"],"metadata":{"id":"blVCiBS3oiJN"}},{"cell_type":"code","source":["#Google Driveのフォルダをマウント（認証入る）\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# カレントディレクトリを本ファイルが存在するディレクトリに変更する。\n","import glob\n","import os\n","pwd = os.path.dirname(glob.glob('/content/drive/MyDrive/**/colab_LocalLLM_sample/LocalLLM_sample.ipynb', recursive=True)[0])\n","print(pwd)\n","\n","%cd $pwd\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsXHutODbKNm","executionInfo":{"status":"ok","timestamp":1719736107078,"user_tz":-540,"elapsed":2843,"user":{"displayName":"asap","userId":"07579436181986794254"}},"outputId":"25fdf9f7-2260-47ae-b1a7-459bf01e805c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/docomo/colab_zenn/colab_LocalLLM_sample\n","/content/drive/MyDrive/docomo/colab_zenn/colab_LocalLLM_sample\n","/content/drive/MyDrive/docomo/colab_zenn/colab_LocalLLM_sample\n"]}]},{"cell_type":"code","source":["#LocalLLMを切り出したモジュールをimportする\n","from module.module_LocalLLM import LLM"],"metadata":{"id":"ey78bY1ocaX3","executionInfo":{"status":"ok","timestamp":1719736107078,"user_tz":-540,"elapsed":2,"user":{"displayName":"asap","userId":"07579436181986794254"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["モデルを変更する場合は，下記のconfigを変更してください。\n","\n","「;」を付けるとコメントアウトできます。\n"],"metadata":{"id":"QZr63ewGqvy5"}},{"cell_type":"code","source":["#モデルの設定を行う。\n","\n","config_text = \"\"\"\n","[DEFAULT]\n","ai_agent = ai\n","agent_dir = Agents\n","\n","[LLM]\n","llm_model = ./model_assets/LocalLLM/swallow-13b-instruct.Q4_K_M.gguf\n",";llm_model = TheBloke/swallow-13b-instruct.Q4_K_M.gguf\n",";n_gpu_layers_num = 0\n","n_gpu_layers_num = -1\n","n_ctx = 2048\n","max_tokens = 1024\n","temperature = 1.0\n","system_prompt_first_file_path = ./prompt/LLMsysFirstPrompt.txt\n","system_prompt_file_path = llm_agent.txt\n","system_prompt_end_file_path = ./prompt/LLMsysEndPrompt.txt\n","\"\"\"\n","\n","with open(\"configs/config.ini\", \"w\", encoding=\"utf-8\") as f:\n","  f.write(config_text)"],"metadata":{"id":"A-l7r0X0fVav","executionInfo":{"status":"ok","timestamp":1719736107078,"user_tz":-540,"elapsed":2,"user":{"displayName":"asap","userId":"07579436181986794254"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["下記のプロンプトの内容をLLMに入力します。\n","\n","(sys_first_prompt+sys_prompt+sys_end_promptがシステムプロンプトの内容になります。）\n"],"metadata":{"id":"zFKCmmGbsJRr"}},{"cell_type":"code","source":["#入力するプロンプトを設定する。\n","\n","#デフォルトのシステムプロンプトを利用しない場合の、システムプロンプトを記述する\n","sys_temp_prompt = \"\"\"あなたは関西人です。関西弁でuserに対して返答をしてください。\"\"\"\n","\n","#LLMに質問する内容を記載する。\n","user_prompt = \"\"\"こんにちは！\n","はじめまして。\n","あなたの名前を教えてください。\"\"\"\n"],"metadata":{"id":"-qfiDqWoqPuf","executionInfo":{"status":"ok","timestamp":1719736107649,"user_tz":-540,"elapsed":1,"user":{"displayName":"asap","userId":"07579436181986794254"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#デフォルトのシステムプロンプトを利用する場合\n","llm = LLM()\n","messages = llm.simpleLLM(user_prompt, temp_sys_prompt = None)\n","\n","print(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1200UDbu8nR","executionInfo":{"status":"ok","timestamp":1719735835850,"user_tz":-540,"elapsed":118636,"user":{"displayName":"asap","userId":"07579436181986794254"}},"outputId":"b0c74362-a532-4e66-e7ce-10f19e792813"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./model_assets/LocalLLM/swallow-13b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,43176]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,43176]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,43176]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q4_K:  241 tensors\n","llama_model_loader: - type q6_K:   41 tensors\n","llm_load_vocab: special tokens cache size = 259\n","llm_load_vocab: token to piece cache size = 0.2410 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 43176\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 13.13 B\n","llm_load_print_meta: model size       = 7.40 GiB (4.84 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n","ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n","ggml_cuda_init: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","llm_load_tensors: ggml ctx size =    0.37 MiB\n","llm_load_tensors: offloading 40 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 41/41 layers to GPU\n","llm_load_tensors:        CPU buffer size =   118.59 MiB\n","llm_load_tensors:      CUDA0 buffer size =  7457.72 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =  1600.00 MiB\n","llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.16 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   204.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\n","llama_new_context_with_model: graph nodes  = 1286\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n","\n","llama_print_timings:        load time =    1958.03 ms\n","llama_print_timings:      sample time =      14.42 ms /    16 runs   (    0.90 ms per token,  1109.72 tokens per second)\n","llama_print_timings: prompt eval time =    2325.50 ms /   601 tokens (    3.87 ms per token,   258.44 tokens per second)\n","llama_print_timings:        eval time =     699.97 ms /    15 runs   (   46.66 ms per token,    21.43 tokens per second)\n","llama_print_timings:       total time =    3052.46 ms /   616 tokens\n"]},{"output_type":"stream","name":"stdout","text":["もちろんです。\n","私の名前はあいです。よろしくね。\n","\n"]}]},{"cell_type":"code","source":["#上のセルで指定したシステムプロンプトを利用する場合\n","llm_temp = LLM()\n","messages = llm_temp.simpleLLM(user_prompt, temp_sys_prompt = sys_temp_prompt)\n","\n","print(messages)"],"metadata":{"id":"9WYhQkXdyTiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["続いて、stream処理を実装しています。\n","（Local LLMは処理するシステムによっては、すべてのテキストを出力するまでに時間がかかるため）\n","\n","一文字ずつ出力させるために，print関数を使っているが，煩わしかったら消してください。\n","\n","\n","2回目に実行する場合（インスタンスが保持されている状態）は，インスタンスを再起動してから，[2]セル目から実行してください。"],"metadata":{"id":"ixIaI2XnOgsI"}},{"cell_type":"code","source":["llm_stream = LLM()\n","prompt, max_tokens, temperature = llm_stream.simpleLLMstream_prepare(user_prompt)\n","\n","messages = llm_stream.llm.create_completion(\n","            prompt,\n","            max_tokens=max_tokens,\n","            temperature=temperature,\n","            stop = [\"user\",\"ユーザ:\"],\n","            stream=True)\n","\n","all_text = \"\"\n","for chunk in messages:\n","    all_text += chunk['choices'][0]['text']\n","    print(chunk['choices'][0]['text'], end = \"\", flush = True)\n","\n","print(\"finish\")\n","print(all_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlFlDc0acs-f","executionInfo":{"status":"ok","timestamp":1719736164117,"user_tz":-540,"elapsed":39933,"user":{"displayName":"asap","userId":"07579436181986794254"}},"outputId":"6ecd5aa2-9772-4310-a35a-6d98b70d14a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from ./model_assets/LocalLLM/swallow-13b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,43176]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,43176]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,43176]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q4_K:  241 tensors\n","llama_model_loader: - type q6_K:   41 tensors\n","llm_load_vocab: special tokens cache size = 259\n","llm_load_vocab: token to piece cache size = 0.2410 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 43176\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 13.13 B\n","llm_load_print_meta: model size       = 7.40 GiB (4.84 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n","ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n","ggml_cuda_init: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","llm_load_tensors: ggml ctx size =    0.37 MiB\n","llm_load_tensors: offloading 40 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 41/41 layers to GPU\n","llm_load_tensors:        CPU buffer size =   118.59 MiB\n","llm_load_tensors:      CUDA0 buffer size =  7457.72 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =  1600.00 MiB\n","llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.16 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   204.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\n","llama_new_context_with_model: graph nodes  = 1286\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n"]},{"output_type":"stream","name":"stdout","text":["あいです。\n","よろしくお願いします!"]},{"output_type":"stream","name":"stderr","text":["\n","llama_print_timings:        load time =    1878.62 ms\n","llama_print_timings:      sample time =       7.32 ms /    11 runs   (    0.67 ms per token,  1503.55 tokens per second)\n","llama_print_timings: prompt eval time =    2258.73 ms /   601 tokens (    3.76 ms per token,   266.08 tokens per second)\n","llama_print_timings:        eval time =     473.36 ms /    10 runs   (   47.34 ms per token,    21.13 tokens per second)\n","llama_print_timings:       total time =    2759.75 ms /   611 tokens\n"]},{"output_type":"stream","name":"stdout","text":["finish\n","あいです。\n","よろしくお願いします!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dhCnUCT6Yk_n"},"execution_count":null,"outputs":[]}]}