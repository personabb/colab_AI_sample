{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22039,"status":"ok","timestamp":1741113948499,"user":{"displayName":"asap","userId":"07579436181986794254"},"user_tz":-540},"id":"lkIgZFtRR4cN","outputId":"85202824-483a-48af-95f5-569f4fba4fff"},"outputs":[],"source":["%cd /content\n","!git lfs install\n","!git clone https://huggingface.co/spaces/ASLP-lab/DiffRhythm\n","%cd /content/DiffRhythm\n","!ls\n","!mkdir example\n","!apt install espeak-ng"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhcs96dYT7HS","outputId":"a319cd0b-5b83-4478-815b-b78aa2da7f59"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1741113759606,"user":{"displayName":"asap","userId":"07579436181986794254"},"user_tz":-540},"id":"f8_d_JJRSFaQ"},"outputs":[],"source":["config_text = \"\"\"\n","import argparse\n","import torch\n","import torchaudio\n","import os\n","\n","# DiffRhythm関連\n","from diffrhythm.infer.infer_utils import (\n","    prepare_model,\n","    get_reference_latent,\n","    get_lrc_token,\n","    get_style_prompt,\n","    get_negative_style_prompt\n",")\n","from diffrhythm.infer.infer import inference\n","\n","def infer_music(\n","    cfm_model,\n","    vae_model,\n","    tokenizer,\n","    muq,\n","    device,\n","    lrc_text,\n","    ref_audio_path,\n","    steps=32,\n","    file_type=\"wav\",\n","    max_frames=2048\n","):\n","    # ここから先は前回と同様の処理\n","    sway_sampling_coef = -1 if steps < 32 else None\n","    lrc_prompt, start_time = get_lrc_token(lrc_text, tokenizer, device)\n","    style_prompt = get_style_prompt(muq, ref_audio_path)\n","    negative_style_prompt = get_negative_style_prompt(device)\n","    latent_prompt = get_reference_latent(device, max_frames)\n","\n","    generated_waveform = inference(\n","        cfm_model=cfm_model,\n","        vae_model=vae_model,\n","        cond=latent_prompt,\n","        text=lrc_prompt,\n","        duration=max_frames,\n","        style_prompt=style_prompt,\n","        negative_style_prompt=negative_style_prompt,\n","        steps=steps,\n","        sway_sampling_coef=sway_sampling_coef,\n","        start_time=start_time,\n","        file_type=file_type\n","    )\n","    return generated_waveform\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser(\n","        description=\"DiffRhythmを用いて歌詞と参照音声から音楽を生成するスクリプト\"\n","    )\n","    parser.add_argument(\"--lyrics\", type=str, default=\"example/eg.lrc\",\n","                        help=\"歌詞ファイル（LRC形式）のパス\")\n","    parser.add_argument(\"--ref_audio\", type=str, default=\"example/pray.mp3\",\n","                        help=\"参照音声ファイルのパス（10秒以上推奨）\")\n","    parser.add_argument(\"--output\", type=str, default=\"output.wav\",\n","                        help=\"生成結果を保存するファイル名 (wav/mp3/oggなど拡張子は適宜変更)\")\n","    parser.add_argument(\"--steps\", type=int, default=32,\n","                        help=\"Diffusionのステップ数（推奨: 32以上）\")\n","    parser.add_argument(\"--file_type\", type=str, default=\"wav\",\n","                        choices=[\"wav\", \"mp3\", \"ogg\"],\n","                        help=\"出力フォーマット\")\n","    parser.add_argument(\"--max_frames\", type=int, default=2048,\n","                        help=\"最大フレーム数（曲の長さに相当）\")\n","    parser.add_argument(\"--device\", type=str, default=\"cuda\",\n","                        help=\"推論に使用するデバイス (cuda, mps, or cpu)\")\n","\n","    args = parser.parse_args()\n","\n","    # デバイス判定\n","    # --device で指定された値を元に、使えるかどうかチェック\n","    if args.device == \"mps\":\n","        if torch.backends.mps.is_available():\n","            print(\"MPSを使用します。\")\n","            device = \"mps\"\n","        else:\n","            print(\"Warning: MPSがサポートされていない環境のようです。CPUを使用します。\")\n","            device = \"cpu\"\n","    elif args.device == \"cuda\":\n","        if torch.cuda.is_available():\n","            device = \"cuda\"\n","        else:\n","            print(\"Warning: CUDAがサポートされていない環境のようです。CPUを使用します。\")\n","            device = \"cpu\"\n","    else:\n","        device = \"cpu\"\n","\n","    print(f\"Using device: {device}\")\n","\n","    # モデル準備\n","    print(\"Loading models...\")\n","    cfm, tokenizer, muq, vae = prepare_model(device)\n","\n","    # torch.compile が使用できる環境なら最適化 (PyTorch 2.0以降)\n","    try:\n","        cfm = torch.compile(cfm)\n","    except:\n","        pass\n","\n","    # 歌詞(LRC)読み込み\n","    with open(args.lyrics, 'r', encoding='utf-8') as f:\n","        lrc_text = f.read()\n","\n","    # 音楽生成\n","    print(\"Generating music...\")\n","    generated_waveform = infer_music(\n","        cfm_model=cfm,\n","        vae_model=vae,\n","        tokenizer=tokenizer,\n","        muq=muq,\n","        device=device,\n","        lrc_text=lrc_text,\n","        ref_audio_path=args.ref_audio,\n","        steps=args.steps,\n","        file_type=args.file_type,\n","        max_frames=args.max_frames\n","    )\n","\n","    # 出力を保存\n","    sample_rate = generated_waveform[0]\n","    waveform_data = generated_waveform[1]\n","\n","    # waveformをtorch.Tensorに変換（もしnumpyなら）\n","    if not isinstance(waveform_data, torch.Tensor):\n","        waveform_tensor = torch.from_numpy(waveform_data).float()\n","    else:\n","        waveform_tensor = waveform_data\n","\n","    # torchaudioは (channels, samples)の形が必要なので転置\n","    waveform_tensor = waveform_tensor.T  # (4194304, 2) → (2, 4194304)\n","\n","    # ファイルに保存\n","    torchaudio.save(args.output, waveform_tensor, sample_rate)\n","    print(\"Done.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","\n","\"\"\"\n","\n","with open(\"main.py\", \"w\", encoding=\"utf-8\") as f:\n","  f.write(config_text)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1741113760127,"user":{"displayName":"asap","userId":"07579436181986794254"},"user_tz":-540},"id":"tZ-NwsqfS08C"},"outputs":[],"source":["config_text = \"\"\"\n","[00:10.00] The cold wind pierces through my heart\n","[00:13.20] The blurry streetlights hide my tears\n","[00:16.85] The moment I knew what loneliness was\n","[00:20.40] I felt like I found a piece of the future\n","[00:24.15] Beyond the locked door\n","[00:27.65] I hear a whispering dream\n","[00:31.30] I want to believe, but I'm so afraid\n","[00:34.90] Reaching out with trembling hands\n","[00:38.55] Under the stardust night, I make a wish\n","[00:42.10] That your smile will never fade away\n","[00:45.75] Holding onto strength within the fleeting moments\n","[00:49.25] I will keep chasing the light, again and again\n","[00:52.00] Guided by the signpost soaked in rain\n","[00:55.30] I trace back the memories from afar\n","[00:58.90] The unseen future makes me anxious\n","[01:02.50] But a small flame flickers deep in my heart\n","[01:06.25] There's no dream that’s out of reach\n","[01:09.75] Because you were the one who showed me\n","[01:13.40] I won’t forget, no matter when\n","[01:16.95] Your voice will always lead me\n","\n","\"\"\"\n","\n","with open(\"example/eg.lrc\", \"w\", encoding=\"utf-8\") as f:\n","  f.write(config_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85460,"status":"ok","timestamp":1741113846111,"user":{"displayName":"asap","userId":"07579436181986794254"},"user_tz":-540},"id":"WqbSpNhKTQDB","outputId":"ad60eca2-129e-4692-a7be-001631fbead6"},"outputs":[],"source":["!python main.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pd1TDvMSW3Lm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPJFaAI3QJuyYtm4m8AzBj+","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
